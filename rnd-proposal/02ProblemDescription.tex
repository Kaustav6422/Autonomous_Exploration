\section{Problem Description}

\subsection{Problem Addressed}
Our objective is to implement exploration scheme for ground mobile robot, which enables it to
autonomously explore the given indoor environment using its perception sensors.
Using these laser scans the robot should be able to build a 2D map of the surroundings.
The end result of this endeavor will be a module which is able to devise an exploration plan.
The efficiency of the exploration strategy is also a concern which can be measured using metrics
proposed in \cite{Yan2015}.
\par
Formally \textit{Occupancy Grid Maps} represent the occupation probability of each zone of the
environment within a grid \cite{Juliae2012}. One of the reasons of choosing a strategy is that
it can work with occupancy grids. Exploration strategy also is aimed to be generic enough to work
with any mobile robots in any scenario, rather be it industrial robots or domestic assistance robots

\subsection{Approach}
\begin{itemize}

\item We need to choose metrics of comparison for different exploration algorithms. In domestic
scenario it would be required of the system that it can explore the complete floor of the hospital.
simultaneously for an autonomous robot working in an industry precise map building with accurate
machine and work stations positions is crucial, so we need to find a trade-off between colliding
metrics of interest proposed by \cite{Yan2015}.

\item We also need to setup a simulation environments, which are intended for testing. The chosen
algorithms will be first trialled and tested in simulation and then on individual robots. the target
is to find a general exploration algorithm which can work in any virtual or real surroundings.

\item Based on the assessment presented by \cite{Juliae2012} and \cite{Yan2015} we need to chose
algorithms according to conflicting interests in the metrics. As an example we require fast
exploration in huge environments like hospitals and according to Juliae et. al. Nearest Frontier is
the best candidate for that, but for a precision critical problem accuracy in SLAM is more
important. In the SLAM integrated exploration, we need loop closures and multiple readings of
interesting features(land marks) of the map.

 \item One target of this endeavor is to solve this complex control problem by Reinforcement
Learning(RL). We need chose an RL algorithm which can work with continuous action space since
position and heading are continuous variables. Policy based methods are able to handle continuous
control problem a good example of such method is Deterministic Policy Gradient(DPG). The state space
of such a system will comprise of Occupancy Grid, sensor readings and uncertainty measure of the
SLAM.
\item Simultaneously we need to reward the system based on these parts, for instance to introduce
obstacle avoidance we can reward the system based on the distance it kept from obstacles. Also, we
can reward the system based on reduction in SLAM uncertainty. these are some integral parts of the
reward system, nevertheless their are more Key Performance Indicators(KPIs) Which can be included.

following are some KPIs explained in detail by Yan et. al. :
\begin{itemize}
  \item exploration time
  \item exploration cost
  \item map completeness
  \item exploration efficiency
  \item map quality
\end{itemize}

\item We aim to implement this reward system based on the map assessment KPIs, for this we can
implement a KPI evaluation node in ROS(Robot Operation System). This node will also be used in
assessment of the other exploration strategies. For example the map evaluation node can try to find
a closing boundary in an incomplete map, till it does not find an enclosing boundary it will output
reward '0'. As soon as the node finds the enclosing boundary it can reward the system +1.

\item At the end the chosen RL algorithm must be compared with other best contenders of their domain.
\end{itemize}

% \subsection{Use Cases}

\subsection{Expected Results}
\begin{itemize}
	\item Minimum:
	\begin{itemize}
		\item Autonomous Exploration with best algorithms accessed by chosen metrics.
	\end{itemize}
	\item Expected:
	In addition to the above:
	\begin{itemize}
		\item Autonomous Exploration with Reinforcement Learning.
	\end{itemize}
	\item Maximum:
	In addition to the above:
	\begin{itemize}
		\item Comparison of both RL vs. cost-utility
	\end{itemize}
\end{itemize}
