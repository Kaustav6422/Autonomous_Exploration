\section{Problem Description}

\subsection{Problem Addressed}
Our objective is to implement exploration scheme for ground mobile robot, which enables it to
autonomously explore the given indoor environment using its perception sensors.
Using these laser scans the robot should be able to build a 2D map of the surroundings.
The end result of this endeavor will be a module which is able to devise an exploration plan.
The efficiency of the exploration strategy is also a concern which can be measured using metrics
proposed in \cite{Yan2015}.
\par
Formally \textit{Occupancy Grid Maps} represent the occupation probability of each zone of the
environment within a grid \cite{Juliae2012}. One of the reasons of choosing a strategy is that
it can work with occupancy grids. The mobile robots for which the autonomous exploration is needed
are care-o-bot and you-bot which are actively used by RoboCup @Home and @Work team respectively at
Bonn-Rhein-Sieg University of Applied Science.

\subsection{Approach}
\begin{itemize}

\item We need to choose metrics of comparison for @Home or @Work scenarios. Mostly in at @Home
competitions max map coverage is required in least amount of time. In @Work competitions we need
accurate maps, with reduced uncertainty, so we need to choose between colliding metrics of interest
proposed by \cite{Yan2015}.

\item We also need to setup a simulation environments, which are intended for testing. The chosen
algorithms will be first trialled and tested in simulation and then on individual robots (care-o-bot
or you-bot).

\item Base on the assessment presented in the by \cite{Juliae2012} and \cite{Yan2015}
we need chose algorithms according to conflicting interests in the metrics. As an example we require
fast exploration in @Home and according to Juliae et. al. Nearest Frontier is the best candidate for
that.

 \item One target of this endeavor is to solve this complex control problem by Reinforcement
Learning(RL). We need chose an RL algorithm which can work with continuous action space since
position and heading are continuous variables. Also, we need to tune the algorithm parameters to
work in individual scenarios of @Home and @Work.

\item At the end the chosen RL algorithm must be compared with other best contenders of their domain.
\end{itemize}

% \subsection{Use Cases}

\subsection{Expected Results}
\begin{itemize}
	\item Minimum:
	\begin{itemize}
		\item Autonomous Exploration with best algorithms accessed by chosen metrics.
	\end{itemize}
	\item Expected:
	In addition to the above:
	\begin{itemize}
		\item Autonomous Exploration with Reinforcement Learning.
	\end{itemize}
	\item Maximum:
	In addition to the above:
	\begin{itemize}
		\item Comparison of both RL vs. cost-utility
	\end{itemize}
\end{itemize}
